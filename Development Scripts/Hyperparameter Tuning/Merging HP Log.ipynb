{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1qUjqieHyfNnBYKuca_LfNQE3NONVq161","authorship_tag":"ABX9TyP/86w/kAzUnEqdlnCu5QQ+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"takE2BL7K9eI"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"lC-zmFXrOn-u","executionInfo":{"status":"ok","timestamp":1720247609867,"user_tz":-420,"elapsed":39229,"user":{"displayName":"Muhammad Arif Nuriman","userId":"00956089797213815660"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"841fb094-7bdb-48ba-cd90-5f599edd3e70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import re\n","import random\n","import math\n","import itertools\n","import pprint\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","import tensorflow_datasets as tfds\n","from sklearn.model_selection import train_test_split\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","drive_path = '/content/drive/MyDrive/Kuliah/Tugas Akhir/Final Project Shared Folder'\n","data_path = \"Dataset/Data Versioning/\"\n","model_path = \"Model/ML Model/\"\n","data_version = \"Trained_Oversampled.csv\"\n","base_url = \"https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?CycleBeginYear=2017\"\n","dataset_names = ['Demographics', 'Dietary', 'Examination', 'Laboratory', 'Questionnaire']\n","hyperparameter_log_path = \"Notebook/Arif's Workspace/Hyperparameter Tuning/Log\"\n","hyperparameter_log_full_refresh = \"V5\"\n","hyperparameter_log_increment = [\n","    \"V1/hyperparameter_tuning_log_2.csv\"\n","]\n","hyperparameter_log_full_refresh_custom = [\n","    \"V1/hyperparameter_tuning_log_1.csv\",\n","]\n","\n","output_file = os.path.join(drive_path, hyperparameter_log_path+\"/Merged/V5.csv\")\n","\n","run_type = 'full_refresh'\n","# run_type = 'full_refresh_custom'\n","# run_type = 'increment'\n"]},{"cell_type":"markdown","source":["# ETL"],"metadata":{"id":"BMcECmguK7ra"}},{"cell_type":"code","source":["def full_refresh():\n","\n","  file_paths = []\n","\n","  for root, dirs, files in os.walk(os.path.join(drive_path, hyperparameter_log_path+\"/\"+hyperparameter_log_full_refresh)):\n","    for i, file in enumerate(files):\n","      file_path = os.path.join(root, file)\n","      log = pd.read_csv(file_path)\n","\n","      print(\"Merging File\", file, \"With Size\", str(log.shape[0]), \"Row\")\n","\n","      if i == 0:\n","        log_merged = pd.read_csv(file_path)\n","        continue\n","\n","      log_merged = pd.concat([log_merged, log])\n","\n","  log_merged = log_merged.drop_duplicates()\n","  print(\"-\"*10)\n","  print(\"Successfully Merged\", str(log_merged.shape[0]), \"Row Data\")\n","  print(\"Saving to\", output_file, \"...\")\n","  print(\"-\"*10)\n","\n","  log_merged.to_csv(output_file, index=False)\n","\n","  print(\"Success!\")\n","  print(\"-\"*10)\n","\n","  return log_merged"],"metadata":{"id":"SuLbk1-hPA-k","executionInfo":{"status":"ok","timestamp":1720247609868,"user_tz":-420,"elapsed":17,"user":{"displayName":"Muhammad Arif Nuriman","userId":"00956089797213815660"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def full_refresh_custom():\n","\n","  file_paths = []\n","\n","  for i, file in enumerate(hyperparameter_log_full_refresh_custom):\n","    file_path = os.path.join(drive_path, os.path.join(hyperparameter_log_path, file))\n","    log = pd.read_csv(file_path)\n","\n","    print(\"Merging File\", file, \"With Size\", str(log.shape[0]), \"Row\")\n","\n","    if i == 0:\n","      log_merged = log\n","      continue\n","\n","    log_merged = pd.concat([log_merged, log])\n","\n","  log_merged = log_merged.drop_duplicates()\n","  print(\"-\"*10)\n","  print(\"Successfully Merged\", str(log_merged.shape[0]), \"Row Data\")\n","  print(\"Saving to\", output_file, \"...\")\n","  print(\"-\"*10)\n","\n","  log_merged.to_csv(output_file, index=False)\n","\n","  print(\"Success!\")\n","  print(\"-\"*10)\n","\n","  return log_merged"],"metadata":{"id":"8sS2BrIWLpMa","executionInfo":{"status":"ok","timestamp":1720247609868,"user_tz":-420,"elapsed":14,"user":{"displayName":"Muhammad Arif Nuriman","userId":"00956089797213815660"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def incremental():\n","\n","  log_merged = pd.read_csv(output_file)\n","  print(\"Existing File Has\", str(log_merged.shape[0]), \"Row -\", output_file)\n","  print(\"-\"*10)\n","\n","  file_paths = []\n","\n","  for i, file in enumerate(hyperparameter_log_increment):\n","    file_path = os.path.join(drive_path, os.path.join(hyperparameter_log_path, file))\n","    log = pd.read_csv(file_path)\n","\n","    print(\"Merging File\", file, \"With Size\", str(log.shape[0]), \"Row\")\n","\n","    log_merged = pd.concat([log_merged, log])\n","\n","  log_merged = log_merged.drop_duplicates()\n","  print(\"-\"*10)\n","  print(\"Successfully Merged\", str(log_merged.shape[0]), \"Row Data\")\n","  print(\"Saving to\", output_file, \"...\")\n","  print(\"-\"*10)\n","\n","  log_merged.to_csv(output_file, index=False)\n","\n","  print(\"Success!\")\n","  print(\"-\"*10)\n","\n","  return log_merged\n"],"metadata":{"id":"QyTk07OvOydO","executionInfo":{"status":"ok","timestamp":1720247609869,"user_tz":-420,"elapsed":13,"user":{"displayName":"Muhammad Arif Nuriman","userId":"00956089797213815660"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Run ETL"],"metadata":{"id":"73x8KH4KQuvt"}},{"cell_type":"code","source":["if run_type == 'full_refresh':\n","  full_refresh()\n","elif run_type == 'full_refresh_custom':\n","  full_refresh_custom()\n","elif run_type == 'increment':\n","  incremental()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoyGs3SeQtqK","executionInfo":{"status":"ok","timestamp":1720247614735,"user_tz":-420,"elapsed":4875,"user":{"displayName":"Muhammad Arif Nuriman","userId":"00956089797213815660"}},"outputId":"461744be-c56e-43fc-eb78-12e80a93f2d2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Merging File hyperparameter_tuning_log (2).csv With Size 81 Row\n","Merging File hyperparameter_tuning_log (3).csv With Size 11 Row\n","Merging File hyperparameter_tuning_log (4).csv With Size 188 Row\n","Merging File hyperparameter_tuning_log (5).csv With Size 74 Row\n","Merging File hyperparameter_tuning_log (6).csv With Size 74 Row\n","Merging File hyperparameter_tuning_log (7).csv With Size 75 Row\n","Merging File hyperparameter_tuning_log (8).csv With Size 73 Row\n","Merging File hyperparameter_tuning_log (9).csv With Size 71 Row\n","Merging File hyperparameter_tuning_log (11).csv With Size 72 Row\n","Merging File hyperparameter_tuning_log (12).csv With Size 72 Row\n","----------\n","Successfully Merged 791 Row Data\n","Saving to /content/drive/MyDrive/Kuliah/Tugas Akhir/Final Project Shared Folder/Notebook/Arif's Workspace/Hyperparameter Tuning/Log/Merged/V5.csv ...\n","----------\n","Success!\n","----------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VB9K58gM6Apy","executionInfo":{"status":"ok","timestamp":1720247614737,"user_tz":-420,"elapsed":21,"user":{"displayName":"Muhammad Arif Nuriman","userId":"00956089797213815660"}}},"execution_count":5,"outputs":[]}]}